# document_cleaner_enhanced.py
"""
å¢å¼ºç‰ˆæ–‡æ¡£æ¸…æ´—å·¥å…· - é›†æˆNLTKå’ŒspaCy
æä¾›æ›´æ™ºèƒ½çš„å¥å­åˆ†å‰²ã€å®ä½“è¯†åˆ«ã€å…³é”®è¯æå–
"""
import re
from typing import List, Dict, Optional
from bs4 import BeautifulSoup

# å¯é€‰å¯¼å…¥ - å¦‚æœåº“ä¸å­˜åœ¨ä¼šå›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False
    print("âš ï¸ chardetæœªå®‰è£…ï¼Œä½¿ç”¨UTF-8ç¼–ç ï¼ˆå¯èƒ½å‡ºç°ä¹±ç ï¼‰")

try:
    import nltk
    from nltk.tokenize import sent_tokenize
    HAS_NLTK = True
    # è‡ªåŠ¨ä¸‹è½½å¿…è¦çš„æ•°æ®
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("ğŸ“¥ ä¸‹è½½NLTK punktæ•°æ®...")
        nltk.download('punkt', quiet=True)
        nltk.download('punkt_tab', quiet=True)
except ImportError:
    HAS_NLTK = False
    print("âš ï¸ NLTKæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•æ­£åˆ™åˆ†å‰²å¥å­")

try:
    import spacy
    HAS_SPACY = True
    # å°è¯•åŠ è½½æ¨¡å‹
    try:
        nlp_zh = spacy.load("zh_core_web_sm")
        nlp_en = spacy.load("en_core_web_sm")
        print("âœ… spaCyæ¨¡å‹å·²åŠ è½½ï¼ˆä¸­æ–‡+è‹±æ–‡ï¼‰")
    except OSError:
        print("âš ï¸ spaCyæ¨¡å‹æœªå®‰è£…")
        print("   å®‰è£…å‘½ä»¤:")
        print("   python -m spacy download zh_core_web_sm")
        print("   python -m spacy download en_core_web_sm")
        HAS_SPACY = False
        nlp_zh = None
        nlp_en = None
except ImportError:
    HAS_SPACY = False
    nlp_zh = None
    nlp_en = None
    print("âš ï¸ spaCyæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨NLPåŠŸèƒ½")


class EnhancedDocumentCleaner:
    """å¢å¼ºç‰ˆæ–‡æ¡£æ¸…æ´—ç±» - æ”¯æŒNLTKå’ŒspaCy"""
    
    def __init__(self, config: dict = None):
        self.config = config or {}
        self.remove_patterns = self.config.get('remove_patterns', [])
        self.min_line_length = self.config.get('min_line_length', 10)
        
        # åŠŸèƒ½æ ‡å¿—
        self.use_chardet = HAS_CHARDET
        self.use_nltk = HAS_NLTK
        self.use_spacy = HAS_SPACY
        
        print(f"ğŸ“Š å¢å¼ºåŠŸèƒ½çŠ¶æ€:")
        print(f"   Chardet: {'âœ…' if self.use_chardet else 'âŒ'}")
        print(f"   NLTK: {'âœ…' if self.use_nltk else 'âŒ'}")
        print(f"   spaCy: {'âœ…' if self.use_spacy else 'âŒ'}")
    
    def load_file_with_encoding(self, file_path: str) -> str:
        """
        æ™ºèƒ½åŠ è½½æ–‡ä»¶ - è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶å†…å®¹ï¼ˆstrï¼‰
        """
        if self.use_chardet:
            # ä½¿ç”¨chardetè‡ªåŠ¨æ£€æµ‹ç¼–ç 
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result['confidence']
            
            if confidence < 0.7:
                print(f"âš ï¸ ç¼–ç æ£€æµ‹ç½®ä¿¡åº¦è¾ƒä½: {confidence:.2f}, ä½¿ç”¨UTF-8")
                encoding = 'utf-8'
            
            try:
                return raw_data.decode(encoding)
            except:
                # å›é€€åˆ°UTF-8
                return raw_data.decode('utf-8', errors='ignore')
        else:
            # å›é€€åˆ°åŸºç¡€æ–¹æ³•
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
    
    def clean_text(self, text: str, source_type: str = 'txt') -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼ˆä¸åŸºç¡€ç‰ˆæœ¬ç›¸åŒï¼‰"""
        if not text or not text.strip():
            return ""
            
        if source_type in ['html', 'htm']:
            text = self._clean_html(text)
        elif source_type == 'pdf':
            text = self._clean_pdf(text)
        
        text = self._general_clean(text)
        
        return text.strip()
    
    def _clean_html(self, html_content: str) -> str:
        """æ¸…æ´—HTMLå†…å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            for script in soup(['script', 'style', 'meta', 'link', 'noscript']):
                script.decompose()
            
            text = soup.get_text(separator='\n')
            lines = [line.strip() for line in text.split('\n')]
            lines = [line for line in lines if line]
            
            return '\n'.join(lines)
        except Exception as e:
            print(f"HTMLæ¸…æ´—å¤±è´¥: {e}")
            return html_content
    
    def _clean_pdf(self, text: str) -> str:
        """æ¸…æ´—PDFæ–‡æœ¬"""
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            if not line:
                continue
            
            if re.match(r'^[-\s]*\d+[-\s]*$', line):
                continue
            if re.match(r'^Page\s+\d+\s+of\s+\d+$', line, re.IGNORECASE):
                continue
            if re.match(r'^Copyright.*$', line, re.IGNORECASE):
                continue
            if re.match(r'^Â©.*$', line):
                continue
            if 'All rights reserved' in line:
                continue
            
            if len(line) < self.min_line_length:
                continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _general_clean(self, text: str) -> str:
        """é€šç”¨æ–‡æœ¬æ¸…æ´—"""
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        for pattern in self.remove_patterns:
            text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)
        
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        return text
    
    def extract_metadata_enhanced(self, text: str, source_path: str, language: str = 'auto') -> dict:
        """
        å¢å¼ºç‰ˆå…ƒæ•°æ®æå– - ä½¿ç”¨spaCy
        
        Args:
            text: æ–‡æ¡£å†…å®¹
            source_path: æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ï¼ˆ'zh', 'en', 'auto'ï¼‰
            
        Returns:
            åŒ…å«ä¸°å¯Œå…ƒæ•°æ®çš„å­—å…¸
        """
        metadata = {
            'source': source_path,
            'word_count': len(text.split()),
            'char_count': len(text)
        }
        
        # åŸºç¡€æ ‡é¢˜æå–
        lines = text.split('\n')[:10]
        for line in lines:
            line = line.strip()
            if len(line) > 10 and len(line) < 200:
                if not line[0].isdigit():
                    metadata['title'] = line
                    break
        
        # åŸºç¡€æ‘˜è¦
        clean_text = ' '.join(text.split())
        metadata['summary'] = clean_text[:200] + '...' if len(clean_text) > 200 else clean_text
        
        # å¦‚æœæœ‰spaCyï¼Œæå–æ›´å¤šä¿¡æ¯
        if self.use_spacy:
            try:
                # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
                if language == 'auto':
                    # ç®€å•å¯å‘å¼ï¼šæ£€æŸ¥ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
                    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text[:1000]))
                    is_chinese = chinese_chars > 50
                    nlp = nlp_zh if is_chinese else nlp_en
                else:
                    nlp = nlp_zh if language == 'zh' else nlp_en
                
                if nlp is None:
                    return metadata
                
                # åªå¤„ç†å‰5000å­—ç¬¦ï¼ˆæ€§èƒ½è€ƒè™‘ï¼‰
                doc = nlp(text[:5000])
                
                # æå–å‘½åå®ä½“
                entities = {
                    'persons': [],
                    'organizations': [],
                    'locations': [],
                    'products': [],
                    'other': []
                }
                
                for ent in doc.ents:
                    if ent.label_ in ['PERSON', 'PER']:
                        entities['persons'].append(ent.text)
                    elif ent.label_ in ['ORG', 'ORGANIZATION']:
                        entities['organizations'].append(ent.text)
                    elif ent.label_ in ['GPE', 'LOC', 'LOCATION']:
                        entities['locations'].append(ent.text)
                    elif ent.label_ in ['PRODUCT', 'WORK_OF_ART']:
                        entities['products'].append(ent.text)
                    else:
                        entities['other'].append(ent.text)
                
                # å»é‡
                for key in entities:
                    entities[key] = list(set(entities[key]))[:5]  # æœ€å¤šä¿ç•™5ä¸ª
                
                metadata['entities'] = entities
                
                # æå–å…³é”®è¯ï¼ˆåè¯å’Œä¸“æœ‰åè¯ï¼‰
                keywords = []
                for token in doc:
                    if token.pos_ in ['NOUN', 'PROPN'] and len(token.text) > 2:
                        keywords.append(token.text)
                
                # ç»Ÿè®¡é¢‘ç‡ï¼Œå–top 10
                from collections import Counter
                keyword_freq = Counter(keywords)
                metadata['keywords'] = [kw for kw, _ in keyword_freq.most_common(10)]
                
            except Exception as e:
                print(f"âš ï¸ spaCyå…ƒæ•°æ®æå–å¤±è´¥: {e}")
        
        return metadata


def smart_chunk_text_enhanced(
    text: str,
    chunk_size: int = 800,
    overlap: int = 150,
    min_chunk_size: int = 100,
    respect_sentence: bool = True,
    language: str = 'auto'
) -> List[str]:
    """
    å¢å¼ºç‰ˆæ™ºèƒ½æ–‡æœ¬åˆ†å— - ä½¿ç”¨NLTKæˆ–spaCy
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        chunk_size: ç›®æ ‡å—å¤§å°
        overlap: é‡å å¤§å°
        min_chunk_size: æœ€å°å—å¤§å°
        respect_sentence: æ˜¯å¦å°Šé‡å¥å­è¾¹ç•Œ
        language: è¯­è¨€ï¼ˆ'zh', 'en', 'auto'ï¼‰
        
    Returns:
        æ–‡æœ¬å—åˆ—è¡¨
    """
    if not text or len(text.strip()) < min_chunk_size:
        return []
    
    if not respect_sentence:
        # ç®€å•å­—ç¬¦åˆ†å—
        return _chunk_by_chars(text, chunk_size, overlap, min_chunk_size)
    
    # å°è¯•ä½¿ç”¨spaCyï¼ˆæœ€ä½³ï¼‰
    if HAS_SPACY:
        try:
            # æ£€æµ‹è¯­è¨€
            if language == 'auto':
                chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text[:1000]))
                is_chinese = chinese_chars > 50
                nlp = nlp_zh if is_chinese else nlp_en
            else:
                nlp = nlp_zh if language == 'zh' else nlp_en
            
            if nlp is not None:
                return _chunk_by_spacy(text, chunk_size, overlap, min_chunk_size, nlp)
        except Exception as e:
            print(f"âš ï¸ spaCyåˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°NLTK: {e}")
    
    # å›é€€åˆ°NLTK
    if HAS_NLTK:
        try:
            return _chunk_by_nltk(text, chunk_size, overlap, min_chunk_size)
        except Exception as e:
            print(f"âš ï¸ NLTKåˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™: {e}")
    
    # æœ€ç»ˆå›é€€åˆ°æ­£åˆ™
    return _chunk_by_sentences_regex(text, chunk_size, overlap, min_chunk_size)


def _chunk_by_spacy(text: str, chunk_size: int, overlap: int, min_size: int, nlp) -> List[str]:
    """
    ä½¿ç”¨spaCyåˆ†å— - æœ€æ™ºèƒ½çš„æ–¹å¼
    è€ƒè™‘å¥å­è¾¹ç•Œå’Œå®ä½“è¾¹ç•Œ
    """
    # åˆ†æ®µå¤„ç†ï¼ˆspaCyå¤„ç†é•¿æ–‡æœ¬è¾ƒæ…¢ï¼‰
    max_chars_per_batch = 100000
    all_sentences = []
    
    for i in range(0, len(text), max_chars_per_batch):
        batch = text[i:i + max_chars_per_batch]
        doc = nlp(batch)
        all_sentences.extend([sent.text for sent in doc.sents])
    
    return _build_chunks_from_sentences(all_sentences, chunk_size, overlap, min_size)


def _chunk_by_nltk(text: str, chunk_size: int, overlap: int, min_size: int) -> List[str]:
    """ä½¿ç”¨NLTKåˆ†å— - è¾ƒå¥½çš„æ–¹å¼"""
    try:
        sentences = sent_tokenize(text)
        return _build_chunks_from_sentences(sentences, chunk_size, overlap, min_size)
    except Exception as e:
        print(f"NLTKåˆ†å—é”™è¯¯: {e}")
        return _chunk_by_sentences_regex(text, chunk_size, overlap, min_size)


def _chunk_by_sentences_regex(text: str, chunk_size: int, overlap: int, min_size: int) -> List[str]:
    """ä½¿ç”¨æ­£åˆ™åˆ†å— - åŸºç¡€æ–¹å¼"""
    sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s+)', text)
    
    full_sentences = []
    for i in range(0, len(sentences), 2):
        if i + 1 < len(sentences):
            full_sentences.append(sentences[i] + sentences[i + 1])
        else:
            full_sentences.append(sentences[i])
    
    return _build_chunks_from_sentences(full_sentences, chunk_size, overlap, min_size)


def _build_chunks_from_sentences(sentences: List[str], chunk_size: int, overlap: int, min_size: int) -> List[str]:
    """ä»å¥å­åˆ—è¡¨æ„å»ºåˆ†å—"""
    chunks = []
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        sentence_len = len(sentence)
        
        if current_size + sentence_len > chunk_size and current_chunk:
            # ä¿å­˜å½“å‰å—
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text) >= min_size:
                chunks.append(chunk_text)
            
            # å¼€å§‹æ–°å—ï¼Œä¿ç•™overlap
            if overlap > 0:
                overlap_text = chunk_text[-overlap:] if len(chunk_text) > overlap else chunk_text
                current_chunk = [overlap_text, sentence]
                current_size = len(overlap_text) + sentence_len
            else:
                current_chunk = [sentence]
                current_size = sentence_len
        else:
            current_chunk.append(sentence)
            current_size += sentence_len
    
    # æ·»åŠ æœ€åä¸€å—
    if current_chunk:
        chunk_text = ' '.join(current_chunk)
        if len(chunk_text) >= min_size:
            chunks.append(chunk_text)
    
    return chunks


def _chunk_by_chars(text: str, chunk_size: int, overlap: int, min_size: int) -> List[str]:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†å—ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = min(start + chunk_size, text_len)
        chunk = text[start:end].strip()
        
        if len(chunk) >= min_size:
            chunks.append(chunk)
        
        start = end - overlap
        if start <= 0:
            break
    
    return chunks


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¢å¼ºç‰ˆæ¸…æ´—å™¨
    cleaner = EnhancedDocumentCleaner()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    Dr. Smithä»OpenAIå­¦ä¹ äº†GPT-4çš„æ¶æ„è®¾è®¡ã€‚ä»–è¯´ï¼š"U.S.A.çš„AIå‘å±•å¾ˆå¿«ï¼"
    å¾®è½¯Azureæä¾›äº†APIæœåŠ¡ã€‚ç‰ˆæœ¬v2.1.3å·²ç»å‘å¸ƒã€‚
    Kubernetesæ˜¯ä¸€ä¸ªå®¹å™¨ç¼–æ’å¹³å°ï¼Œç”±Googleå¼€å‘ã€‚
    """
    
    # æµ‹è¯•å…ƒæ•°æ®æå–
    print("\n" + "="*60)
    print("ğŸ“Š å…ƒæ•°æ®æå–æµ‹è¯•")
    print("="*60)
    
    metadata = cleaner.extract_metadata_enhanced(test_text, "test.txt")
    
    print(f"\næ ‡é¢˜: {metadata.get('title', 'N/A')}")
    print(f"å­—æ•°: {metadata['word_count']}")
    print(f"å­—ç¬¦æ•°: {metadata['char_count']}")
    
    if 'entities' in metadata:
        print("\nğŸ·ï¸ å®ä½“è¯†åˆ«:")
        for entity_type, entities in metadata['entities'].items():
            if entities:
                print(f"  {entity_type}: {', '.join(entities)}")
    
    if 'keywords' in metadata:
        print(f"\nğŸ”‘ å…³é”®è¯: {', '.join(metadata['keywords'])}")
    
    # æµ‹è¯•åˆ†å—
    print("\n" + "="*60)
    print("âœ‚ï¸ æ™ºèƒ½åˆ†å—æµ‹è¯•")
    print("="*60)
    
    chunks = smart_chunk_text_enhanced(test_text, chunk_size=100, overlap=20)
    
    print(f"\nç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—:\n")
    for i, chunk in enumerate(chunks, 1):
        print(f"å— {i}: {chunk[:80]}...")
        print()
