# requirements_enhanced.txt
# 增强版向量数据库ETL Pipeline依赖
# 包含所有高级文本处理库

# ============================================
# 核心依赖（必需）
# ============================================
qdrant-client>=1.7.0
requests>=2.31.0
pyyaml>=6.0
pypdf>=3.17.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
tqdm>=4.66.0
flask>=3.0.0
python-dotenv>=1.0.0

# ============================================
# 增强功能库（推荐）
# ============================================

# 编码检测 - 避免中文乱码（强烈推荐）
chardet>=5.2.0

# NLTK - 更好的句子分割（推荐）
nltk>=3.8.1

# spaCy - 高级NLP功能（可选，但效果显著）
# 实体识别、关键词提取、语义分析
spacy>=3.7.0

# 文件类型检测（可选）
# 注意：需要系统安装 libmagic
# Ubuntu: sudo apt-get install libmagic1
# macOS: brew install libmagic
# Windows: 相对复杂，建议跳过
# python-magic>=0.4.27

# ============================================
# spaCy语言模型（单独安装）
# ============================================
# 安装spaCy后，运行以下命令下载模型：
# 
# 中文模型（~50MB）:
#   python -m spacy download zh_core_web_sm
# 
# 英文模型（~12MB）:
#   python -m spacy download en_core_web_sm
# 
# 如果需要更大更准确的模型:
#   zh_core_web_md (中等，~100MB)
#   zh_core_web_lg (大型，~500MB)
#   en_core_web_md (中等，~40MB)
#   en_core_web_lg (大型，~500MB)

# ============================================
# 可选：更高级的功能
# ============================================

# TextBlob - 情感分析、翻译（可选）
# textblob>=0.17.0

# jieba - 中文分词（如果需要更好的中文处理）
# jieba>=0.42.0

# langdetect - 语言检测（如果处理多语言文档）
# langdetect>=1.0.9

# ============================================
# 开发和测试依赖（可选）
# ============================================
# pytest>=7.4.0
# black>=23.0.0
# flake8>=6.0.0
