#!/usr/bin/env python3
# ingest_qdrant_v2.py
"""
RAG æ–‡æ¡£æ‘„å–è„šæœ¬ - Azure OpenAI ç‰ˆæœ¬
æ”¯æŒå¢å¼ºçš„æ–‡æœ¬å¤„ç†å’Œ Azure OpenAI embedding
"""
import uuid
import argparse
import sys
from pathlib import Path
from typing import List, Dict
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆingest_qdrant_v2.py çš„çˆ¶çº§çš„çˆ¶çº§ï¼‰
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®å’Œå·¥å…·
try:
    from config.settings import get_settings
    from utils.helpers import (
        setup_logger,
        PerformanceMetrics,
        DiskCache,
        show_progress,
        file_batch_iterator
    )
    from document_cleaner_enhanced import (
        EnhancedDocumentCleaner,
        smart_chunk_text_enhanced
    )
    from azure_embedding import AzureOpenAIEmbedding
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½åœ¨æ­£ç¡®ä½ç½®")
    sys.exit(1)


class DocumentIngester:
    """æ–‡æ¡£æ‘„å–å™¨ - æ”¯æŒ Azure OpenAI"""
    
    def __init__(self, config_path: str = "config/config_azure.yaml"):
        """
        åˆå§‹åŒ–æ‘„å–å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        self.settings = get_settings(config_path)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(
            name="DocumentIngester",
            level=self.settings.logging.level,
            log_file=self.settings.logging.file,
            console_output=self.settings.logging.console_output,
            colored_output=self.settings.logging.colored_output
        )
        
        self.logger.info("="*60)
        self.logger.info("ğŸš€ RAG æ–‡æ¡£æ‘„å–ç³»ç»Ÿå¯åŠ¨")
        self.logger.info("="*60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger.info("ğŸ“¦ åˆå§‹åŒ–ç»„ä»¶...")
        
        # 1. Qdrant å®¢æˆ·ç«¯
        self.qdrant = QdrantClient(
            host=self.settings.qdrant.host,
            port=self.settings.qdrant.port
        )
        self.logger.info(f"âœ… Qdrant: {self.settings.qdrant.host}:{self.settings.qdrant.port}")
        
        # 2. Azure OpenAI Embedding
        try:
            self.embedder = AzureOpenAIEmbedding(
                max_retries=self.settings.embedding.max_retries,
                timeout=self.settings.embedding.timeout
            )
            self.logger.info(f"âœ… Azure OpenAI Embedding")
            
            # æµ‹è¯•è¿æ¥
            #if not self.embedder.test_connection():
                #raise Exception("Azure OpenAI è¿æ¥æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"âŒ Azure OpenAI åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # 3. æ–‡æ¡£æ¸…æ´—å™¨
        self.cleaner = EnhancedDocumentCleaner({
            'remove_patterns': self.settings.cleaning.custom_patterns,
            'min_line_length': self.settings.cleaning.min_line_length
        })
        self.logger.info(f"âœ… æ–‡æ¡£æ¸…æ´—å™¨ (å¢å¼ºåŠŸèƒ½å·²å¯ç”¨)")
        
        # 4. ç¼“å­˜ç³»ç»Ÿ
        if self.settings.processing.enable_caching:
            self.cache = DiskCache(self.settings.processing.cache_dir)
            self.logger.info(f"âœ… Embedding ç¼“å­˜: {self.settings.processing.cache_dir}")
        else:
            self.cache = None
        
        # 5. æ€§èƒ½ç›‘æ§
        self.metrics = PerformanceMetrics()
        
        # ç¡®ä¿é›†åˆå­˜åœ¨
        self._ensure_collection()
        
        self.logger.info("âœ¨ åˆå§‹åŒ–å®Œæˆ\n")
        
    def _ensure_collection(self):
        """ç¡®ä¿ Qdrant é›†åˆå­˜åœ¨"""
        collection_name = self.settings.qdrant.collection_name
        
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = self.qdrant.get_collections().collections
            exists = any(c.name == collection_name for c in collections)
            
            if not exists:
                self.logger.info(f"ğŸ“¦ åˆ›å»ºé›†åˆ: {collection_name}")
                
                self.qdrant.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=self.settings.qdrant.vector_size,
                        distance=Distance.COSINE
                    )
                )
                self.logger.info(f"âœ… é›†åˆåˆ›å»ºæˆåŠŸ")
            else:
                self.logger.info(f"âœ… é›†åˆå·²å­˜åœ¨: {collection_name}")
                
        except Exception as e:
            self.logger.error(f"âŒ é›†åˆåˆ›å»ºå¤±è´¥: {e}")
            raise
            
    def process_document(
        self,
        file_path: Path,
        category: str = None
    ) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            category: æ–‡æ¡£åˆ†ç±»
            
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        with self.metrics.timer('document_processing'):
            try:
                # 1. åŠ è½½æ–‡ä»¶
                with self.metrics.timer('file_loading'):
                    content = self.cleaner.load_file_with_encoding(str(file_path))
                    
                if not content or len(content) < 10:
                    self.logger.warning(f"âš ï¸  æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­: {file_path.name}")
                    return []
                
                # 2. æ¸…æ´—æ–‡æœ¬
                with self.metrics.timer('text_cleaning'):
                    extension = file_path.suffix.lower().lstrip('.')
                    cleaned = self.cleaner.clean_text(content, extension)
                    
                # 3. æå–å…ƒæ•°æ®
                with self.metrics.timer('metadata_extraction'):
                    metadata = self.cleaner.extract_metadata_enhanced(
                        cleaned,
                        str(file_path)
                    )
                
                # 4. è‡ªåŠ¨åˆ†ç±»ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
                if not category:
                    detected_category = self.settings.auto_categorize(cleaned)
                    category = detected_category.name
                    self.logger.info(f"   è‡ªåŠ¨åˆ†ç±»: {category}")
                
                # 5. åˆ†å—
                with self.metrics.timer('text_chunking'):
                    chunks = smart_chunk_text_enhanced(
                        cleaned,
                        chunk_size=self.settings.chunking.chunk_size,
                        overlap=self.settings.chunking.overlap,
                        min_chunk_size=self.settings.chunking.min_chunk_size,
                        respect_sentence=self.settings.chunking.respect_sentence,
                        language=self.settings.chunking.language
                    )
                
                if not chunks:
                    self.logger.warning(f"âš ï¸  æ— æ³•åˆ†å—: {file_path.name}")
                    return []
                
                # 6. æ„å»ºæ–‡æ¡£å—
                documents = []
                for i, chunk in enumerate(chunks):
                    doc = {
                        'id': f"{file_path.stem}_{i}",
                        'text': chunk,
                        'metadata': {
                            'source': str(file_path.name),
                            'category': category,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'file_type': extension,
                            **metadata
                        }
                    }
                    documents.append(doc)
                
                self.logger.info(f"âœ… {file_path.name}: {len(chunks)} å—")
                self.metrics.increment('documents_processed')
                self.metrics.increment('chunks_created', len(chunks))
                
                return documents
                
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥ {file_path.name}: {e}")
                self.metrics.increment('documents_failed')
                if not self.settings.processing.skip_errors:
                    raise
                return []
                
    def embed_documents(self, documents: List[Dict]) -> List[Dict]:
        """
        ä¸ºæ–‡æ¡£ç”Ÿæˆ embeddings
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å¸¦ embedding çš„æ–‡æ¡£åˆ—è¡¨
        """
        with self.metrics.timer('embedding'):
            # æ”¶é›†éœ€è¦ embedding çš„æ–‡æœ¬
            texts_to_embed = []
            cached_indices = []
            
            for i, doc in enumerate(documents):
                text = doc['text']
                
                # æ£€æŸ¥ç¼“å­˜
                if self.cache:
                    cached_emb = self.cache.get(text)
                    if cached_emb is not None:
                        doc['vector'] = cached_emb
                        cached_indices.append(i)
                        continue
                
                texts_to_embed.append((i, text))
            
            if cached_indices:
                self.logger.info(f"   ğŸ’¾ ä½¿ç”¨ç¼“å­˜: {len(cached_indices)} ä¸ª")
                self.metrics.increment('cache_hits', len(cached_indices))
            
            # æ‰¹é‡ embedding
            if texts_to_embed:
                self.logger.info(f"   ğŸ”„ Embedding: {len(texts_to_embed)} ä¸ª...")
                
                indices, texts = zip(*texts_to_embed)
                
                try:
                    embeddings = self.embedder.embed_batch(
                        list(texts),
                        batch_size=self.settings.embedding.batch_size,
                        show_progress=False
                    )
                    
                    # åˆ†é… embeddings å¹¶ç¼“å­˜
                    for idx, emb in zip(indices, embeddings):
                        documents[idx]['vector'] = emb
                        
                        # ç¼“å­˜
                        if self.cache:
                            self.cache.set(documents[idx]['text'], emb)
                    
                    self.metrics.increment('embeddings_generated', len(embeddings))
                    
                except Exception as e:
                    self.logger.error(f"âŒ Embedding å¤±è´¥: {e}")
                    raise
            
            return documents
                
    def upsert_to_qdrant(self, documents: List[Dict]):
        """
        ä¸Šä¼ æ–‡æ¡£åˆ° Qdrantï¼ˆåˆ†æ‰¹ä¸Šä¼ ï¼Œé¿å… payload è¿‡å¤§ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        import time
        
        with self.metrics.timer('qdrant_upsert'):
            try:
                # æ„å»º points
                all_points = []
                for doc in documents:
                    point = PointStruct(
                        id=str(uuid.uuid5(uuid.NAMESPACE_DNS, doc['id'])),
                        vector=doc['vector'],
                        payload={
                            'text': doc['text'],
                            'metadata': doc['metadata']
                        }
                    )
                    all_points.append(point)
                
                # åˆ†æ‰¹ä¸Šä¼ ï¼ˆé¿å… payload è¿‡å¤§ï¼‰
                # Qdrant é™åˆ¶ï¼š32MB per request
                # æ¯ä¸ªå‘é‡çº¦ 3072 * 4 bytes = 12KB + text + metadata â‰ˆ 40KB
                # å®‰å…¨æ‰¹æ¬¡ï¼š500 ä¸ªç‚¹ â‰ˆ 20MB
                batch_size = 500
                total_uploaded = 0
                
                self.logger.info(f"   å‡†å¤‡ä¸Šä¼  {len(all_points)} ä¸ªå‘é‡ï¼ˆåˆ† {(len(all_points)-1)//batch_size + 1} æ‰¹ï¼‰")
                
                for i in range(0, len(all_points), batch_size):
                    batch = all_points[i:i + batch_size]
                    batch_num = i // batch_size + 1
                    total_batches = (len(all_points) - 1) // batch_size + 1
                    
                    self.logger.info(f"   ğŸ“¤ ä¸Šä¼ æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªå‘é‡)")
                    
                    # æ‰¹é‡ä¸Šä¼ 
                    self.qdrant.upsert(
                        collection_name=self.settings.qdrant.collection_name,
                        points=batch
                    )
                    
                    total_uploaded += len(batch)
                    
                    # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼ˆé¿å…è¿‡è½½ï¼‰
                    if i + batch_size < len(all_points):
                        time.sleep(1)
                
                self.logger.info(f"âœ… å…¨éƒ¨ä¸Šä¼ å®Œæˆ: {total_uploaded} ä¸ªå‘é‡")
                self.metrics.increment('vectors_upserted', total_uploaded)
                
            except Exception as e:
                self.logger.error(f"âŒ Qdrant ä¸Šä¼ å¤±è´¥: {e}")
                raise
                    
    def ingest_directory(
        self,
        directory: Path,
        category: str = None,
        recursive: bool = True
    ):
        """
        æ‘„å–æ•´ä¸ªç›®å½•
        
        Args:
            directory: ç›®å½•è·¯å¾„
            category: æ–‡æ¡£åˆ†ç±»
            recursive: æ˜¯å¦é€’å½’å¤„ç†
        """
        self.logger.info(f"ğŸ“‚ å¤„ç†ç›®å½•: {directory}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        file_extensions = ['.txt', '.md', '.pdf', '.html', '.htm']
        
        # æ”¶é›†æ–‡ä»¶
        file_batches = list(file_batch_iterator(
            directory,
            file_extensions,
            batch_size=self.settings.processing.batch_size,
            recursive=recursive
        ))
        
        total_files = sum(len(batch) for batch in file_batches)
        self.logger.info(f"ğŸ“Š æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶")
        
        if total_files == 0:
            self.logger.warning("âš ï¸  æœªæ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            return
        
        # å¤„ç†æ–‡ä»¶
        for batch in show_progress(
            file_batches,
            desc="å¤„ç†æ–‡ä»¶æ‰¹æ¬¡",
            total=len(file_batches)
        ):
            # 1. å¤„ç†æ–‡æ¡£
            all_documents = []
            for file_path in batch:
                docs = self.process_document(file_path, category)
                all_documents.extend(docs)
            
            if not all_documents:
                continue
            
            # 2. ç”Ÿæˆ embeddings
            try:
                all_documents = self.embed_documents(all_documents)
            except Exception as e:
                self.logger.error(f"âŒ Embedding æ‰¹æ¬¡å¤±è´¥: {e}")
                if not self.settings.processing.skip_errors:
                    raise
                continue
            
            # 3. ä¸Šä¼ åˆ° Qdrant
            try:
                self.upsert_to_qdrant(all_documents)
            except Exception as e:
                self.logger.error(f"âŒ Qdrant ä¸Šä¼ æ‰¹æ¬¡å¤±è´¥: {e}")
                if not self.settings.processing.skip_errors:
                    raise
                continue
        
        # æ‰“å°ç»Ÿè®¡
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ“Š æ‘„å–å®Œæˆ")
        self.logger.info("="*60)
        self.metrics.print_stats()
        
        # ç¼“å­˜ç»Ÿè®¡
        if self.cache:
            self.cache.stats()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="RAG æ–‡æ¡£æ‘„å–å·¥å…· - Azure OpenAI ç‰ˆæœ¬"
    )
    parser.add_argument(
        "directory",
        type=str,
        help="æ–‡æ¡£ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config/config_azure.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--category",
        type=str,
        help="æ–‡æ¡£åˆ†ç±»ï¼ˆå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    parser.add_argument(
        "--no-recursive",
        action="store_true",
        help="ä¸é€’å½’å¤„ç†å­ç›®å½•"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯ç›®å½•
    directory = Path(args.directory)
    if not directory.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        sys.exit(1)
    
    if not directory.is_dir():
        print(f"âŒ ä¸æ˜¯ç›®å½•: {directory}")
        sys.exit(1)
    
    # åˆ›å»ºæ‘„å–å™¨
    try:
        ingester = DocumentIngester(args.config)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    # è¿è¡Œæ‘„å–
    try:
        ingester.ingest_directory(
            directory,
            category=args.category,
            recursive=not args.no_recursive
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        ingester.metrics.print_stats()
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ‘„å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
