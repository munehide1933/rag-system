# compare_versions.py
"""
åŸºç¡€ç‰ˆ vs å¢å¼ºç‰ˆå¯¹æ¯”æµ‹è¯•
ç›´è§‚å±•ç¤ºä½¿ç”¨é«˜çº§åº“çš„æå‡æ•ˆæœ
"""
import time
from document_cleaner import smart_chunk_text as basic_chunk
from document_cleaner_enhanced import smart_chunk_text_enhanced, EnhancedDocumentCleaner

# æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å«å„ç§è¾¹ç•Œæƒ…å†µï¼‰
TEST_CASES = [
    {
        "name": "ç¼©å†™å’Œç‰ˆæœ¬å·",
        "text": """
Dr. Smith works at OpenAI Inc. He said: "The U.S.A. is leading in AI development!"
GPT-4 version v2.1.3 was released. The API v1.0 is deprecated.
Microsoft Azure provides cloud services. Prof. Johnson uses it.
        """.strip()
    },
    {
        "name": "ä¸­è‹±æ–‡æ··åˆ",
        "text": """
OpenAIå‘å¸ƒäº†GPT-4æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé©å‘½æ€§çš„çªç ´ã€‚å¾®è½¯Azureæä¾›APIæœåŠ¡ã€‚
æ ¹æ®Smithåšå£«çš„ç ”ç©¶ï¼ŒU.S.A.åœ¨AIé¢†åŸŸå¤„äºé¢†å…ˆåœ°ä½ã€‚Kubernetesæ˜¯Googleå¼€å‘çš„ã€‚
ç‰ˆæœ¬v2.1.3å·²ç»åœ¨productionç¯å¢ƒä¸­éƒ¨ç½²ã€‚Amazon Web Servicesä¹Ÿåœ¨ä½¿ç”¨ã€‚
        """.strip()
    },
    {
        "name": "æŠ€æœ¯æ–‡æ¡£",
        "text": """
Kubernetes (k8s) is a container orchestration platform. It was developed by Google.
The architecture includes: Master nodes, Worker nodes, and etcd cluster.
Version 1.28.0 introduces new features. The API version is v1.
Configure kubectl using: kubectl config set-context. Prof. Dr. Anderson recommends it.
        """.strip()
    },
    {
        "name": "å¼•å·å’Œçœç•¥å·",
        "text": """
He said: "AI is the future... but we must be careful." Dr. Smith agreed.
"The model achieves 95% accuracy," according to the paper. Prof. Johnson added: "This is impressive."
The system uses ML/DL techniques. Version 2.0... 2.1... and now 2.2 are available.
        """.strip()
    }
]


def test_sentence_splitting():
    """æµ‹è¯•å¥å­åˆ†å‰²è´¨é‡"""
    print("="*80)
    print("ğŸ“ æµ‹è¯•1: å¥å­åˆ†å‰²è´¨é‡")
    print("="*80)
    
    cleaner = EnhancedDocumentCleaner()
    
    for case in TEST_CASES:
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•æ¡ˆä¾‹: {case['name']}")
        print(f"{'='*80}")
        print(f"åŸæ–‡:\n{case['text']}\n")
        
        # åŸºç¡€ç‰ˆæœ¬ï¼ˆæ­£åˆ™åˆ†å‰²ï¼‰
        print("ã€åŸºç¡€ç‰ˆ - æ­£åˆ™åˆ†å‰²ã€‘")
        basic_chunks = basic_chunk(
            case['text'],
            chunk_size=80,
            overlap=10,
            respect_sentence=True
        )
        
        for i, chunk in enumerate(basic_chunks, 1):
            print(f"  å—{i}: {chunk}")
        
        print()
        
        # å¢å¼ºç‰ˆæœ¬ï¼ˆNLTK/spaCyï¼‰
        print("ã€å¢å¼ºç‰ˆ - NLTK/spaCyåˆ†å‰²ã€‘")
        enhanced_chunks = smart_chunk_text_enhanced(
            case['text'],
            chunk_size=80,
            overlap=10,
            respect_sentence=True
        )
        
        for i, chunk in enumerate(enhanced_chunks, 1):
            print(f"  å—{i}: {chunk}")
        
        # å¯¹æ¯”
        print(f"\nğŸ“Š å¯¹æ¯”:")
        print(f"  åŸºç¡€ç‰ˆå—æ•°: {len(basic_chunks)}")
        print(f"  å¢å¼ºç‰ˆå—æ•°: {len(enhanced_chunks)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯åˆ†å‰²ï¼ˆå•å­—æ¯ã€æ•°å­—å—ï¼‰
        basic_errors = sum(1 for c in basic_chunks if len(c.strip()) < 5)
        enhanced_errors = sum(1 for c in enhanced_chunks if len(c.strip()) < 5)
        
        print(f"  åŸºç¡€ç‰ˆé”™è¯¯å—: {basic_errors}")
        print(f"  å¢å¼ºç‰ˆé”™è¯¯å—: {enhanced_errors}")
        
        if enhanced_errors < basic_errors:
            print("  âœ… å¢å¼ºç‰ˆè´¨é‡æ›´å¥½ï¼")
        elif enhanced_errors == basic_errors:
            print("  âš–ï¸ è´¨é‡ç›¸å½“")
        else:
            print("  âš ï¸ åŸºç¡€ç‰ˆæ›´å¥½ï¼ˆç½•è§ï¼‰")
        
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")


def test_metadata_extraction():
    """æµ‹è¯•å…ƒæ•°æ®æå–"""
    print("\n" + "="*80)
    print("ğŸ·ï¸ æµ‹è¯•2: å…ƒæ•°æ®æå–ï¼ˆéœ€è¦spaCyï¼‰")
    print("="*80)
    
    cleaner = EnhancedDocumentCleaner()
    
    if not cleaner.use_spacy:
        print("âš ï¸ spaCyæœªå®‰è£…ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
        print("   å®‰è£…: pip install spacy")
        print("   ä¸‹è½½æ¨¡å‹: python -m spacy download zh_core_web_sm")
        return
    
    test_text = """
OpenAIå‘å¸ƒäº†GPT-4æ¨¡å‹ï¼Œç”±Sam Altmané¢†å¯¼ã€‚å¾®è½¯Azureæä¾›APIæ”¯æŒã€‚
Googleçš„Kubernetesç”¨äºå®¹å™¨ç¼–æ’ã€‚Amazon Web Services (AWS)ä¹Ÿå¾ˆæµè¡Œã€‚
Elon Muskåˆ›ç«‹äº†SpaceXå’ŒTeslaã€‚è‹¹æœå…¬å¸çš„iPhoneå¾ˆæˆåŠŸã€‚
    """.strip()
    
    print(f"æµ‹è¯•æ–‡æœ¬:\n{test_text}\n")
    
    # åŸºç¡€ç‰ˆæœ¬ï¼ˆæ— å®ä½“è¯†åˆ«ï¼‰
    print("ã€åŸºç¡€ç‰ˆå…ƒæ•°æ®ã€‘")
    basic_metadata = {
        'word_count': len(test_text.split()),
        'char_count': len(test_text)
    }
    
    print(f"  å­—æ•°: {basic_metadata['word_count']}")
    print(f"  å­—ç¬¦æ•°: {basic_metadata['char_count']}")
    print("  å®ä½“: âŒ ä¸æ”¯æŒ")
    print("  å…³é”®è¯: âŒ ä¸æ”¯æŒ")
    
    print()
    
    # å¢å¼ºç‰ˆæœ¬ï¼ˆspaCyå®ä½“è¯†åˆ«ï¼‰
    print("ã€å¢å¼ºç‰ˆå…ƒæ•°æ®ã€‘")
    enhanced_metadata = cleaner.extract_metadata_enhanced(test_text, "test.txt")
    
    print(f"  å­—æ•°: {enhanced_metadata['word_count']}")
    print(f"  å­—ç¬¦æ•°: {enhanced_metadata['char_count']}")
    
    if 'entities' in enhanced_metadata:
        print("\n  ğŸ·ï¸ å®ä½“è¯†åˆ«:")
        for entity_type, entities in enhanced_metadata['entities'].items():
            if entities:
                print(f"    {entity_type}: {', '.join(entities)}")
    
    if 'keywords' in enhanced_metadata:
        print(f"\n  ğŸ”‘ å…³é”®è¯: {', '.join(enhanced_metadata['keywords'][:10])}")
    
    print("\nâœ… å¢å¼ºç‰ˆæä¾›äº†ä¸°å¯Œçš„å…ƒæ•°æ®ï¼Œå¯ç”¨äº:")
    print("   - è‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆ")
    print("   - æ™ºèƒ½æœç´¢è¿‡æ»¤")
    print("   - å†…å®¹åˆ†ç±»ä¼˜åŒ–")
    print("   - å®ä½“å…³ç³»å›¾è°±")


def test_performance():
    """æµ‹è¯•æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "="*80)
    print("âš¡ æµ‹è¯•3: æ€§èƒ½å¯¹æ¯”")
    print("="*80)
    
    # ç”Ÿæˆå¤§æ–‡æœ¬
    large_text = " ".join([
        "This is a test sentence. " * 100,
        "Dr. Smith works at OpenAI Inc. " * 100,
        "The version is v1.2.3. " * 100
    ])
    
    print(f"æµ‹è¯•æ–‡æœ¬å¤§å°: {len(large_text)} å­—ç¬¦\n")
    
    # åŸºç¡€ç‰ˆæ€§èƒ½
    print("ã€åŸºç¡€ç‰ˆ - æ­£åˆ™åˆ†å‰²ã€‘")
    start_time = time.time()
    basic_chunks = basic_chunk(large_text, chunk_size=500, overlap=50)
    basic_time = time.time() - start_time
    
    print(f"  æ—¶é—´: {basic_time:.4f} ç§’")
    print(f"  ç”Ÿæˆå—æ•°: {len(basic_chunks)}")
    print(f"  é€Ÿåº¦: {len(large_text)/basic_time:.0f} å­—ç¬¦/ç§’")
    
    print()
    
    # å¢å¼ºç‰ˆæ€§èƒ½
    print("ã€å¢å¼ºç‰ˆ - NLTK/spaCyåˆ†å‰²ã€‘")
    start_time = time.time()
    enhanced_chunks = smart_chunk_text_enhanced(large_text, chunk_size=500, overlap=50)
    enhanced_time = time.time() - start_time
    
    print(f"  æ—¶é—´: {enhanced_time:.4f} ç§’")
    print(f"  ç”Ÿæˆå—æ•°: {len(enhanced_chunks)}")
    print(f"  é€Ÿåº¦: {len(large_text)/enhanced_time:.0f} å­—ç¬¦/ç§’")
    
    print()
    
    # å¯¹æ¯”
    slowdown = enhanced_time / basic_time
    print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"  å¢å¼ºç‰ˆæ…¢äº† {slowdown:.1f}x")
    
    if slowdown < 2:
        print(f"  âœ… æ€§èƒ½æŸå¤±å¯æ¥å—ï¼ˆ<2xï¼‰")
    elif slowdown < 5:
        print(f"  âš ï¸ æ€§èƒ½æŸå¤±ä¸­ç­‰ï¼ˆ2-5xï¼‰")
    else:
        print(f"  âŒ æ€§èƒ½æŸå¤±è¾ƒå¤§ï¼ˆ>5xï¼‰")
    
    print(f"\nğŸ’¡ ç»“è®º:")
    if slowdown < 3:
        print(f"  å»ºè®®ä½¿ç”¨å¢å¼ºç‰ˆ - è´¨é‡æå‡æ˜¾è‘—ï¼Œæ€§èƒ½æŸå¤±å¯æ¥å—")
    else:
        print(f"  æ ¹æ®éœ€æ±‚é€‰æ‹©:")
        print(f"    - é«˜è´¨é‡è¦æ±‚ â†’ å¢å¼ºç‰ˆ")
        print(f"    - é«˜æ€§èƒ½è¦æ±‚ â†’ åŸºç¡€ç‰ˆ")


def test_encoding_detection():
    """æµ‹è¯•ç¼–ç æ£€æµ‹"""
    print("\n" + "="*80)
    print("ğŸ”¤ æµ‹è¯•4: ç¼–ç æ£€æµ‹ï¼ˆéœ€è¦chardetï¼‰")
    print("="*80)
    
    cleaner = EnhancedDocumentCleaner()
    
    if not cleaner.use_chardet:
        print("âš ï¸ chardetæœªå®‰è£…ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
        print("   å®‰è£…: pip install chardet")
        return
    
    import tempfile
    import os
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶ï¼ˆä¸åŒç¼–ç ï¼‰
    test_cases = [
        ("UTF-8", "è¿™æ˜¯UTF-8ç¼–ç çš„ä¸­æ–‡æ–‡æœ¬", 'utf-8'),
        ("GBK", "è¿™æ˜¯GBKç¼–ç çš„ä¸­æ–‡æ–‡æœ¬", 'gbk'),
        ("GB2312", "è¿™æ˜¯GB2312ç¼–ç çš„ä¸­æ–‡æ–‡æœ¬", 'gb2312'),
    ]
    
    print("åˆ›å»ºæµ‹è¯•æ–‡ä»¶...\n")
    
    for name, text, encoding in test_cases:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.txt') as f:
            temp_path = f.name
            f.write(text.encode(encoding))
        
        try:
            # åŸºç¡€ç‰ˆï¼ˆå‡è®¾UTF-8ï¼‰
            try:
                with open(temp_path, 'r', encoding='utf-8') as f:
                    basic_content = f.read()
                basic_success = True
            except:
                basic_content = "âŒ è¯»å–å¤±è´¥ï¼ˆä¹±ç ï¼‰"
                basic_success = False
            
            # å¢å¼ºç‰ˆï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
            enhanced_content = cleaner.load_file_with_encoding(temp_path)
            
            print(f"ã€{name}ç¼–ç ã€‘")
            print(f"  åŸæ–‡: {text}")
            print(f"  åŸºç¡€ç‰ˆ: {basic_content if basic_success else 'âŒ ä¹±ç '}")
            print(f"  å¢å¼ºç‰ˆ: {enhanced_content}")
            print(f"  ç»“æœ: {'âœ… ä¸¤è€…éƒ½æˆåŠŸ' if basic_success else 'âœ… å¢å¼ºç‰ˆä¿®å¤äº†ä¹±ç '}")
            print()
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
    
    print("ğŸ’¡ ç»“è®º:")
    print("  chardetå¯ä»¥è‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œé¿å…ä¹±ç é—®é¢˜")
    print("  ç‰¹åˆ«é€‚åˆå¤„ç†æ¥è‡ªä¸åŒæ¥æºçš„ä¸­æ–‡æ–‡æ¡£")


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€"*40)
    print("åŸºç¡€ç‰ˆ vs å¢å¼ºç‰ˆ - å…¨é¢å¯¹æ¯”æµ‹è¯•")
    print("ğŸš€"*40 + "\n")
    
    print("æœ¬æµ‹è¯•å°†å±•ç¤ºå¯ç”¨é«˜çº§åº“ï¼ˆNLTKã€spaCyã€chardetï¼‰çš„æå‡æ•ˆæœ\n")
    
    tests = [
        ("å¥å­åˆ†å‰²è´¨é‡", test_sentence_splitting),
        ("å…ƒæ•°æ®æå–", test_metadata_extraction),
        ("æ€§èƒ½å¯¹æ¯”", test_performance),
        ("ç¼–ç æ£€æµ‹", test_encoding_detection)
    ]
    
    for i, (name, test_func) in enumerate(tests, 1):
        print(f"\n{'='*80}")
        print(f"æ‰§è¡Œæµ‹è¯• {i}/{len(tests)}: {name}")
        print(f"{'='*80}\n")
        
        try:
            test_func()
        except KeyboardInterrupt:
            print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        if i < len(tests):
            print("\n" + "-"*80)
            input("æŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€é¡¹æµ‹è¯•...")
    
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    print("""
âœ… å¢å¼ºç‰ˆä¼˜åŠ¿:
  1. å¥å­åˆ†å‰²æ›´å‡†ç¡®ï¼ˆå¤„ç†ç¼©å†™ã€ç‰ˆæœ¬å·ã€å¼•å·ï¼‰
  2. ä¸°å¯Œçš„å…ƒæ•°æ®ï¼ˆå®ä½“è¯†åˆ«ã€å…³é”®è¯æå–ï¼‰
  3. è‡ªåŠ¨ç¼–ç æ£€æµ‹ï¼ˆé¿å…ä¸­æ–‡ä¹±ç ï¼‰
  4. æ›´å¥½çš„åˆ†å—è´¨é‡ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼‰

âš ï¸ å¢å¼ºç‰ˆä»£ä»·:
  1. éœ€è¦å®‰è£…é¢å¤–åº“ï¼ˆ200-300MBï¼‰
  2. é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹
  3. å¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼ˆ2-10å€ï¼‰
  4. å†…å­˜å ç”¨å¢åŠ ï¼ˆ+200MBï¼‰

ğŸ’¡ æ¨èé…ç½®:
  - å°è§„æ¨¡æ–‡æ¡£ã€è¿½æ±‚æ€§èƒ½ â†’ åŸºç¡€ç‰ˆ
  - å¤§è§„æ¨¡æ–‡æ¡£ã€è¿½æ±‚è´¨é‡ â†’ å¢å¼ºç‰ˆ
  - ç”Ÿäº§ç¯å¢ƒã€é«˜è´¨é‡è¦æ±‚ â†’ å¢å¼ºç‰ˆï¼ˆç¦»çº¿æ‰¹å¤„ç†ï¼‰
  
ğŸ“¦ å®‰è£…å¢å¼ºåº“:
  pip install chardet nltk spacy
  python -m nltk.downloader punkt punkt_tab
  python -m spacy download zh_core_web_sm
  python -m spacy download en_core_web_sm
    """)


if __name__ == "__main__":
    main()
